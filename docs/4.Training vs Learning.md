# My AI experiments: Training Vs Learning
## Machines aren't learning and maybe they should

This article is part of a series on my [AI experiments](https://medium.com/@geertvandamme/61757b11c902), where I explore some issues I encountered while creating my own, clean, OO Neural network implementation.

## Training

As already mentioned in (3), the current default training algorithms in AI don't feel right. It takes too much training data, too many cycles and way too much energy. But there are more fundamental aspects to training and learning that I want to explore in this article.

In my opinion, training and learning are 2 related concepts, but take a different perspective.
Learning is a more active process than training because it puts the focus on the subject itself. Whereas Training is an external process where one trains the other.

![learning training1.png](images/learning%20training1.png)

I am learning something myself vs I am training the network to perform a certain task.
Even though we use terms like 'machine learning' and 'deep learning', the machines aren't learning. They are being trained to gain a certain capability. 

Learning seems to imply some self-awareness or consciousness. Training is a more passive process where the trainer trains the trainee. Learning is being able to gain knowledge or skills from the feedback from your own actions.  
These actions don't have to take place in real space (robots), they can also happin in some virtual (cyber)space.

That's how we, humans, but also animals, learn throughout our lives. We try out some things and see if they work. If they don't, we try again and learn from this experience. 
AI systems' learning is quite different. They are trained by us to perform a certain task. They are not learning from their own actions. They rely on us to train them instead of learning themselves. Because they don't have a 'Self'.

## Body

Our body, and our ability to move, are the ultimate reasons that certain cognitive subsystems (like memory, perception, attention, reasoning, ...) evolved in living creatures. Moving is easier and more successful when you have perception and memory. 
example? dog walking 
Consciousness and self-awareness are a result of this evolution.

Although consciousness evolved in moving bodies, embodiment is not a strict requirement for intelligence or consciousness. It's not impossible to create a virtual system that does have a notion of 'Self' and might have some sort of consciousness. But current AI systems don't have this self, because they are no individuals. 
A chatbot can talk about himself, and even use verbs in the first person, it's still referring to itself externally.

## Training phase
What's really strange if you start to think about it is...
In the lifetime of a typical neural network, there's a strict distinction between the training phase and the operational phase.
Training and performing are 2 totally distinct things, using (partly) different algorithms, often performed on different computers. 

This is completely different from how humans behave. We mix learning, training and operating all in one big combination (called life).

## Operational phase
After the network is trained, we deploy it into an environment where it can do its work.
But, the most striking part is that, in this operational phase, the network stops learning anything new.
In AI systems there is hardly anything like 'learning on the job', which is often regarded as an invaluable capability in our everyday life. Being able to adapt to change is a major feature of intelligence. 

It seems obvious that an intelligent system should be able to learn continuously during its lifetime and learn from its mistakes and successes, like we do.
Programmatically, it would be quite easy to create a feedback mechanism so that an AI system can learn in the operational stage by using the same techniques used for training.
In reality, we don't see this happen very often. We deliberately create AI systems that don't adapt to change. 

There is some sort of deferred feedback learning, but not continuous. During the operational phase, new data and feedback on generated output is accumulated and incorporated in the new training data set for training a new version of the model. 

todo: illustration

Adapting to change is a major feature of intelligence, and we're deliberately creating AI systems that don't.
There can be several reasons for this.

### Trust
You only want to learn from feedback if you trust your source of information.
You wouldn't want chatGPT to automatically incorporate feedback from unknown users. 
Uitwerken
Least important issue


### Slow learning process

It doesn't really make a difference to learn from a single feedback item, if learning itself is so slow 
Pavlov 


### Individuation

Even if we solve the trust and the slow learning issue, it's not obvious to learn from feedback.
Often, AI systems are not just a single computer. They exist of an array of identical parallel servers all running the exact same model in the operational phase. 
If one of the servers gets useful feedback from its own output, that can be incorporated into the network itself, into the acquired knowledge, that specific instance starts to drift away from the rest of the identical servers.

create individuals, with their own specific knowledge
Replicating this newly acquired knowledge across all instances might not be simple

The most obvious solution is that feedback isn't used to learn, but just stored centrally to be incorporated in a next release after a new training session.

or feedback updates could be sent to a single master instance who gets all the new input and later distributes that to all others 
They are technically identical

Illustration?
link to serialization?

If we allow different instances to learn new things (from feedback) and thus drift apart, and become different individuals, we'll need a communication protocol so that each instance can teach what it has learned to any other instance. 
However, it won't be as simply as just copying some values as it is  now  (#5)

Language
individuality
Culture

illustration: Yes we are all individuals

Learning from mistakes and adapting to change is a feature of intelligence, not met.



