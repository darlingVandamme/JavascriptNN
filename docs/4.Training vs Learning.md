# Training Vs Learning

- Link to 0.AI Experiments

training comes from an external source. I'm training the system.
learning is conscious. Self driven. The system is learning

- there must be an alternative to the current gradient descent based training
- parallel computers with exact clones
- No learning after training phase
- individuation
- cultures

Training and learning are 2 concepts that seem similar, but take a different perspective.   
Learning is a more active process than Training because it puts the focus on the subject itself. Whereas Training is more external.
I am learning something myself vs I am training the network to perform a certain task.

in AI, Focus on training.
In that light, the often used term 'Machine Learning' is a bit misleading because in most cases, the machine doesn't learn anything, but it is trained to gain a certain capability.

Learning seems to imply some self-awareness or consciousness, whereas training implies a more passive process.

## Training phase
What struck me ...  what's a bit strange ....
In AI there's a clear distinction between the training phase and the operational phase.
Those are 2 totally distinct things, often performed on different computers, which seems a bit strange to us. 
It is completely different from how humans behave. We mix learning, training and operating all in one big combination (called life)

## Operational phase
After the network is trained, we deploy it into an environment where it can do it's work.
But, what looks even stranger to us, is that in that phase the network stops learning anything new.
In AI systems there is hardly anything like 'learning on the job', which is often seen as a very useful feature.

It would seem very obvious that an AI system should be able to learn continuously during its lifetime and learn from its mistakes and successes, like we're doing.

In theory, it should be quite easy to create a feedback mechanism so that the AI system could learn in the operational stage by using the same techniques used for training.
In reality, correct me if I'm wrong, we don't see this happen often.

What does happen is learning in batches. Accumulating new data through the process and then incorporating that data in the new training data set and training a whole new model (or new version)

Adapting to change is a major feature of intelligence. And we're deliberately not adapting to change our systems 

There are several reasons for this.

### Trust
You only want to learn from feedback if you trust your source of information.
You wouldn't want chatGPT to automatically incorporate feedback from unknown users. 
Uitwerken
Least important issue


### Slow learning process

It doesn't really make a difference to learn from a single feedback item, if learning itself is so slow 



### Individuation

Even if we solve the trust and the slow learning issue, it's not easy to learn from feedback.
Often, AI systems are not just a single computer. There's an array of identical parallel servers. 
If one of the servers gets useful feedback, that can be incorporated into the network itself, into the acquired knowledge, that specific instance starts to drift away from the rest of the identical servers.
create individuals, with their own specific knowledge
Replicating this newly acquired knowledge across all instances might not be simple

The most obvious solution is that feedback isn't used to learn, but just stored centrally to be incorporated in a next release after new training session

or feedback updates could be send to a single master instance who gets all the new input and later distributes that to all others 
They are technically identical

If we allow different instances to learn new things (from feedback) and thus drift appart, and become different individuals, we'll need a communication protocol so that each instance can teach what it has learned to any other instance. 
However, it won't be as simply as just copying some values as it is  now  (#5)

Language
Culture

Learning from mistakes and adapting to change is a feature of intelligence, not met.




## Gradient descent