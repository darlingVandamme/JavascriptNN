# My AI experiments: Intelligent Behavior

This article is part of a series on my [AI experiments](https://medium.com/@geertvandamme/61757b11c902).

## Small Networks

When I was implementing my OO network, I created a very small dummy network, just as a quick test of my code.  
Training and running the full MNIST character recognition network, every time I changed something would take way too much time.
The test network didn't really do anything useful, but allowed me to test in less than a second that the network could be trained and ran correctly.

The network has 5 input neurons, 8 neurons in a hidden layer and 2 outputs.

todo: image

You can think of it as a system that can check an input pattern of 5 (eg. [1,0,1,0,1] or [1,2,3,4,5]) and answer through the 2 output neurons with a 'yes' ([1,0]) or a 'no' ([0,1]).
Or we could interpret the output as option A vs option B, or whatever.

A binary (yes/no or optionsA/optionB) could also be achieved by using a single output neuron, but by using 2 outputs, the network can also respond with a 'maybe' (like [0.78,0.2]) or 'I don't know' ([0.3,0.25]).

Let's say we train the network to differentiate between patterns [1, 0, 0, 2, x] and [0, 1, 0, 0, x]. I didn't train it on 2 strictly specific patterns. I deliberately put some variation in the inputs, to make the task a bit less obvious.

```javascript
for (let i=1 ; i<1000;i++) {
        net.train([1, 0, 0, 2, 1], [0, 1])
        net.train([1, 0, 0, 2, 0], [0, 1])
        net.train([0, 1, 0, 0, 1], [1, 0])
        net.train([0, 1, 0, 0, 0], [1, 0])
        net.train([0, 1, 0, 0, 0], [1, 0])
}
```

We see that after 1000 training iterations, the network seems able to recognize the learned patterns.
The result is never [1,0], but more like [0.99,0.01] as if there's always a slim chance that the pattern answer is wrong.

```javascript
net.feed([1,0,0,2,1]);  //   result = [ 0.012226437864224872, 0.9874360979774687 ]
net.feed([0,1,0,0,0]);  //   result = [ 0.9907408966905382, 0.009613549027103936 ]
```

Of course, for such a simple task, we don't really need a neural network.
A simple check on the first 2 numbers (or even only the first) in the array could easily solve it. Something like:

```javascript
function testArray(arr){
    if (arr[0] == 0 && arr[1] == 1) return [1,0]
    if (arr[0] == 1 && arr[1] == 0) return [0,1]
    return [0,0]
}
```

But it's interesting to see what happens if we feed the network a pattern that it has never seen before.

```javascript
net.feed([1,1,0,2,0])  //   result = [ 0.03775022017320202, 0.9637238580292489 ]
net.feed([1,1,3,1,1])  //   result = [ 0.32480806390318023, 0.6533205585758098 ]
```

In the first case [1,1,0,2,0] the result is more or less [0,1] but with less confidence than in the first tests. As if it is saying, it looks a lot like a [1,0,0,2,0], but not exactly.

In the second case [1,1,3,1,1] the outcome is [0.32, 0.65], which one could translate to "I don't know, I don't really recognize any pattern that I already learned, but I have a slight preference to optionB".

When the network is trained on only 2 specific patterns, it can correctly identify patterns that look like the ones it was trained on, although it will indicate to be less sure about its answer.

These results are quite remarkable, and would already be a bit more difficult to achieve with a simple compare algorithm.

I was really surprised to see this behavior in such a small network, with only 15 neurons.
Although we can hardly call this simple recognition network intelligent, it's a step in that direction, compared to the simple testArray function.
It does show that it can handle new, previously unseen situations, which actually is a sign of intelligence.

Notice that, even in the description of this very small network, I start using anthropomorphic terms like 'behavior', 'knowing', 'recognizing' and 'saying'.

Also notice that the [creatures with the smallest nervous system](https://en.wikipedia.org/wiki/List_of_animals_by_number_of_neurons) only have a few hundred neurons. 

## Emergent Properties

I am convinced that intelligence and consciousness are concepts that can emerge as a system reaches a certain level of complexity.
A single water molecule is not wet. A dozen molecules are still not wet. But at a certain level, when you have enough molecules, we can call the water wet. Wetness is an emergent property when the number of water molecules is large enough.
Similarly, intelligence and consciousness need a certain level of complexity to become possible.

In my ~~opinion~~, it is theoretically possible to create real intelligent and even conscious AI systems, ~~but we haven't done that yet.~~

Intelligence and consciousness are no binary, yes-or-no concepts, but form a gradient. There is no hard distinction between intelligent and non-intelligent systems.

Some people say that current AI systems, like chatGPT, should not be called intelligent, because they are simply deterministic word predictors. But chatGPT surely shows signs of intelligent behavior.

* numeric vectors. chatGPT doesn't even know it's generating words, only number vectors. However, the result is quite impressive, and in a way, out brain neurons are also only processing numeric values.

Consciousness also appears in different degrees. We see ourselves as more conscious than a dog, although a dog certainly shows some aspects of being a conscious creature.
ChatGPT however, is clearly not conscious.
Consciousness requires an affective and an effective side. It has to be able to perceive, but also to act. ChatGPT can only respond to a given question. It's not an actor on its own.

** image

Is consciousness language based?
Dreams <=> chatGPT hallucinations

Conclusion

