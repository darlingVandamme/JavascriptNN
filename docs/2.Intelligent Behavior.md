# My AI experiments: 15 Neurons, Intelligence, and Consciousness 
## What Small Networks Teach Us About Intelligence

Can the behavior of a very simple neural network teach us something profound on how we think about intelligence and consciousness?

This article is part of a series on my [AI experiments](https://medium.com/@geertvandamme/61757b11c902), where I explore some challenges I encountered while creating my own, clean, OO Neural network implementation. In future articles, we’ll talk about different aspects of my experiments.

## Small Networks

When I was implementing my [OO network], I created a very small dummy network, just as a quick test for the java code (link) I was writing.  
Training and running the full character recognition network, every time I changed something would take up too much time.
The test network didn't really do anything useful, but allowed me to test in less than a second that the network could be trained and ran correctly.

The network has 5 input neurons, 8 neurons in a hidden layer and 2 outputs.

![small network.drawio.png](images/small%20network.drawio.png)

You can think of it as a system that can check an input pattern of length 5 (eg. [1,0,1,0,1] or [1,2,3,4,5]) and answer through the 2 output neurons with a 'yes' ([1,0]) or a 'no' ([0,1]).
Or we could interpret the output as option A vs option B, or whatever.

A binary (yes/no or optionsA/optionB) could also be achieved by using a single output neuron, but by using 2 outputs, the network can also respond with a 'maybe' (like [0.78,0.2]) or 'I don't know' ([0.3,0.25]).

Let's say we train the network to differentiate between patterns [1, 0, 0, 2, x] and [0, 1, 0, 0, x]. I didn't train it on 2 strictly specific patterns. I deliberately put some variation in the inputs, to make the task a bit less obvious.

```javascript
for (let i=1 ; i<1000;i++) {
        net.train([1, 0, 0, 2, 1], [0, 1])
        net.train([1, 0, 0, 2, 0], [0, 1])
        net.train([0, 1, 0, 0, 1], [1, 0])
        net.train([0, 1, 0, 0, 0], [1, 0])
        net.train([0, 1, 0, 0, 0], [1, 0])
}
```

We see that after 1000 training iterations, the network seems able to recognize the learned patterns.
The result is never [1,0], but more like [0.99,0.01] as if there's always a slim chance that the pattern answer is wrong.

```javascript
net.feed([1,0,0,2,1]);  //   result = [ 0.012226437864224872, 0.9874360979774687 ]
net.feed([0,1,0,0,0]);  //   result = [ 0.9907408966905382, 0.009613549027103936 ]
```

Of course, for such a simple task, we don't really need a neural network.
A simple check on the first 2 numbers (or even only the first) in the array could easily solve it. Something like:

```javascript
function testArray(arr){
    if (arr[0] == 0 && arr[1] == 1) return [1,0]
    if (arr[0] == 1 && arr[1] == 0) return [0,1]
    return [0,0]
}
```

But it's interesting to see what happens if we feed the network a pattern that it has never seen before.

```javascript
net.feed([1,1,0,2,0])  //   result = [ 0.03775022017320202, 0.9637238580292489 ]
net.feed([1,1,3,1,1])  //   result = [ 0.32480806390318023, 0.6533205585758098 ]
```

In the first case [1,1,0,2,0] the result is more or less [0,1] but with less confidence than in the first tests. As if it is saying, it looks a lot like a [1,0,0,2,0], but not exactly.

In the second case [1,1,3,1,1] the outcome is [0.32, 0.65], which one could translate to "I don't know, I don't really recognize any pattern that I already learned, but I have a slight preference to optionB".

These results are quite remarkable and would already be a bit more difficult to achieve with a simple compare algorithm.

I was really surprised to see this behavior in such a small network, with only 15 neurons.
Although we can hardly call this simple recognition network intelligent, it's a step in that direction, compared to the simple testArray function.
It does show that it can handle new, previously unseen situations, which one could see as a sign of intelligence.

Notice that, even in the description of this very small network, I start using anthropomorphic terms like 'behavior', 'knowing', 'recognizing' and 'saying'. We use human metaphors because it helps us relate to the system — but it also risks overinterpreting its capacities. Is the system really 'doubting' or is that something we project in the answer it gives.

Is such a tiny network relevant for intelligence? 
Well, the [creatures with the smallest nervous system](https://en.wikipedia.org/wiki/List_of_animals_by_number_of_neurons) like a tardigrade, only have a few hundred neurons. Yet they can sense and respond to stimuli and show some sort of intelligence.  


## Emergent Properties

I am convinced that intelligence and consciousness are concepts that can emerge as a system reaches a certain level of complexity.
Some things (can we call them things?) only exist from the combination of many other items. They emerge from the combination. 
A neighborhood is more than just some people living near each other. A forest is much richer than just the trees. A traffic jam is not just a bunch of cars. Water is wet, but a single water molecule isn't. 
A flock of birds, such as starlings forming intricate murmurations, has some aspects that a single starling doesn't have.
Similarly, intelligence and consciousness need a certain level of complexity to emerge into existence.

Even with just 15 neurons, our small neural network hints at emergent behavior by making decisions about previously unseen input patterns.
Some people say that current AI systems, like chatGPT, should not be called intelligent because they are simply deterministic word predictors. But chatGPT surely shows signs of intelligent behavior.
What level of complexity is required to call something intelligent? At what point can we say that something is conscious?

Although we will discuss some theoretical and practical obstacles in later articles, I think it is theoretically possible to create real intelligent and even conscious AI systems. Some call that AGI, Artificial General Intelligence. But there's general agreement that our current AI systems are not advanced enough to be called AGI.

While small networks like this don’t approach intelligence on a human scale, they illustrate how complexity can yield emergent behavior. This leads us to ask whether increasing complexity could eventually bring about real intelligence and consciousness.
Some will argue that intelligence without understanding or agency is meaningless. But maybe it’s precisely by studying these non-understanding intelligences that we can refine what we mean by understanding.

## Consciousness

Intelligence and consciousness are no binary, yes-or-no concepts. They form some sort of spectrum or gradient. There is no hard distinction between intelligent and non-intelligent systems.
Consciousness and intelligence don’t switch on like a light — they emerge like dawn, gradually, from complexity.

Consciousness also appears in different degrees. We see ourselves as more conscious than a dog, although a dog certainly shows some aspects of being a conscious creature.
ChatGPT however, is clearly not conscious.

Consciousness requires an affective and an effective side. It has to be able to perceive, but also to act. The affective side is the ability to perceive or receive sensory input.
The effective side is the ability to move, to talk, to do things, to have an impact on the world around us. Consciousness arises in the feedback loop of our effect, over our impact, back to our affective input. 
An individual creates its own Self from the perceived consequences of their own actions. We will explore this in more detail in a later article.

ChatGPT, and almost all AI systems, can only respond to a given question. It's not an actor on its own. Even what we now call AI agents, hardly have any real agency. They simply reply to a request, but with a larger time interval.

![The Effective - Affective Feedback Loop](/home/geert/projects/data/ai/src/docs/images/Consciousness.png)

Determining if something is conscious is even harder than deciding if something is intelligent. 
There have been several claims that something (trees, fungi, nature, the universe...) is or could be conscious. Often this is a very subjective and challenging opinion.
It's not always easy to find a good and objective definition or criteria of consciousness.
By looking for such criteria, we run the risk of focusing too narrowly on the aspects of consciousness that we know from our own experience.    
That might make us overlook the potential consciousness of other objects. 

Like Solipsism only sees one single conscious instance in the whole world, me myself, we could miss the potential consciousness of non-people.
We might fail to recognize consciousness because we are interpreting it solely from our own human perspective.
At a certain point, an artificial system will be conscious, and we won't even notice.

If you enjoy this mix of hands-on technicality and big-picture philosophy, there’s more to come. I’ll be sharing new articles in this series soon. Follow me to catch the next chapters.
