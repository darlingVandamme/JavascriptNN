# My AI experiments: Driven or Limited by Technology
## The technologies that enabled AI growth might limit it at the same time

The hardware and software technologies which enabled the recent surge in AI development, could themselves become a major limiting factor. If we want our search for intelligent systems to keep on growing, we will need creativity and radically new approaches. This will only be possible if we're willing to question the current prejudices and dogmas. 

This article is part of a series on my [AI experiments](https://medium.com/@geertvandamme/61757b11c902), where I explore some challenges I encountered while creating my own, clean, OO Neural network implementation. 

## What seems wrong

In this article, I will explore some issues where I think current AI implementations are fundamentally limited.
The way I implemented my own code allows us to deviate from the stereotypical AI designs in a creative and futureproof way.  

## Brute Force Evolution

The recent AI boom is driven by a technological evolution that makes it possible to develop, train, and run the systems we're seeing everywhere now.
The basics of neural networks and the algorithms used to train and run these networks are a few decades old. It's only now that faster hardware, more memory, cloud computing, and faster GPU's arrived at a level that allows us to create increasingly powerful AI solutions for a whole range of problems.
However, the current AI evolution is largely based on quantitative brute-force progress with bigger and more computers, more GPU's, and [larger training datasets](https://medium.com/predict/ai-is-hitting-a-hard-ceiling-it-cant-pass-851f4667d39b).
This brute force approach leads to exponentially more resource-hungry systems. 
There are some [signs](https://www.youtube.com/watch?v=AqwSZEQkknU)  that we're already hitting a limit. The same technology that enabled us to create AI systems is also becoming a hard limiting factor.

## Algorithms and frameworks
I [already mentioned](https://medium.com/@geertvandamme/building-an-object-oriented-neural-network-ee3f4af085b6#c78a) that AI development seems stuck in a local minimum, as we say in typical AI lingo. We made a lot of progress recently, and we know that there's still a lot more room for improvement, but somehow, the current path doesn't seem the most promising. 

If we want a solution to this problem, we'll have to look for qualitative, intelligent growth instead of just brute force.
This will require creative experiments and radical approach changes.
The stereotypical algorithms and frameworks we routinely use, although they delivered some extraordinary results, seem too rigid for real innovation.  
The way we're thinking about AI is based on programming idioms and dogmas that cloud our clear thinking.  
It is my impression that these stereotypes prevent us from being flexible enough to develop radically new, creative architectures and might become a restriction on AI progress.

I guess some major players in AI technology are also researching these alternative approaches. (feel free to contact me ;-)
I didn't find the time yet to explore all these radically new directions that I have in mind, but I'll try to work on it. Some of these ideas might turn out totally useless, others might offer interesting results, and still some others might be part of the fundamental paradigm changes I mentioned above.

## Restricting factors

During my research, I found that most articles and tutorials are based on a number of assumptions, idioms, and conventions.  
Choosing a very specific implementation because of supposed performance gains comes at a cost of extra inflexibility. Creative, out-of-the-box, alternative directions remain unexplored because of our self-imposed rigidity.

### The default matrix algorithms

When exploring the AI tutorials, it struck me that almost all articles and videos use matrix operations to perform the calculations to train and run a neural network.
At the same time, they use fancy diagrams and images explaining the inner workings of a neural network at the neuron level. 
However, the matrix-based code doesn't reflect the neurological explanation at all. 
This discrepancy is striking. Why not simply have your program mimic the original neurological explanation, like I did in my [OO implementation](https://medium.com/@geertvandamme/building-an-object-oriented-neural-network-ee3f4af085b6)?

![Neuron vs Matrix approach](images/Neuro%20vs%20Matrix.drawio.png)

Especially in the context of a tutorial, your code should reflect your basic explanation and not some incomprehensible mathematical equivalent.
The cognitive dissonance between these 2 supposedly equivalent representations is, in the AI development community, mostly solved by appreciating the matrix view as the real solution, the way 'real' developers think. The neuron approach becomes just an illustration. A way of explaining neural networks to people less programming-experienced.
However, it really is the other way around. The basic building blocks of neural networks are the neurons. The matrices are a mathematical projection of the original idea. 

Of course, I know, the matrix algorithms are supposed to give us superior performance. 

Except....  They don't. And they come with their own set of disadvantages.

At least not directly. It only boosts performance because we have libraries that can offload the matrix multiplication to a GPU. (Or in the case of Python, because we have libraries that can offload the calculations to another, decent, programming language.)  

As controversial as it sounds, the matrix algorithms offer no real advantage on their own. 
Especially in the context of tutorials, where raw performance is not a real issue, we should focus on clean and understandable code.

```python
    def feedforward(self, a):
        """Return the output of the network if ``a`` is input."""
        for b, w in zip(self.biases, self.weights):
            a = sigmoid(np.dot(w, a)+b)
        return a
```
This isn't exactly an example of clean readable code. It favors a programming culture of using supposedly clever tricks that may seem intelligent and handy to experienced AI programmers, but are in fact obscuring the real operations. Apart from the bad variable names, the code is also making the basic concepts of layers and neurons implicit. Nothing in the code represents the neuron or layer concept. 

Starting off with a matrix-based implementation, only because it's supposed to make our code run faster, is a clear example of premature optimization.

> "Premature Optimization is the Root of All Evil"
> 
> -Donald Knuth


Of course, the matrix approach isn't useless in AI, as demonstrated in about every current real world AI system. It allows us to execute our calculations on specialized hardware. The performance boost that this hardware delivers can be necessary to come to a workable real-life solution.
But the fact that the matrix algorithms are presented as the default, or even the only viable option, blinds us from other, more flexible, creative alternatives.

### Python as the default AI language

Pythonâ€™s simplicity, extensive library ecosystem, and large community support have helped it become the dominant programming language for AI and data analysis.
Python is a language often favored and taught in academia.

However, Python has notable drawbacks when applied to computationally intensive tasks like training and deploying large neural networks.
The language is slow, ugly (a tiny bit subjective), and, despite [it was initially developed to make programs more readable](https://peps.python.org/pep-0020/), often encourages bad programming habits.
Python favors or even encourages quick and dirty code. At first, that might seem like a good thing for a more creative experimental approach.
But neural networks are computationally intensive, and python is so slow, it quickly forces us to use external libraries and frameworks. 
The reliance on external libraries such as NumPy and TensorFlow has undoubtedly accelerated AI development. However, it makes us dependent on fixed design decisions and abstracts away details, which limits our ability to innovate.

The use of python as the default language for AI development brings along a specific rigid, inflexible [programming culture](https://gist.github.com/RobertAKARobin/a1cba47d62c009a378121398cc5477ea).

> "That's the way we do things over here" is never a good mindset for innovation.

I'm not saying you should use Rust, Go, Java, Node ... instead of python. In the ideal situation, you should pick the language that best suits your needs in a particular case. The fact that python is so prevalent in AI is just unfortunate.

From an educational perspective, it would even be interesting to implement a neural network in SQL, or Excel.

### Aren't GPU's meant for gaming?

As discussed before, we use GPU's because they allow us to perform a certain type of calculations much faster, especially in large networks. 
GPU's absolutely have an immense value in AI development, since they can perform a lot of calculations in parallel.
But it feels wrong when we limit our algorithmic choices based on an architectural hardware preconception.

Using the extra GPU power isn't limited to just matrix multiplications. Other operations can also be performed on GPU's, as long as we can do them in parallel. 
It's highly probable that we can still use this power to perform the neuron-oriented calculations. But that's something I want to explore later.  

## Radical changes

Let's make this a bit more concrete.
What exactly do I mean by the fundamental creative changes I hinted at in the previous section?
What are some of the options that become possible if we allow our architecture to deviate from the matrix normativity?
The following suggestions are some of the ideas that appeared to me while working on my neuron implementation but also based on my understanding of our own brain.

In the field of AI, there are generally two main "schools of thought" regarding the brain analogy:
1. **Brain as a Metaphor**: some researchers consider the brain merely as a useful metaphor. It helps explain AI concepts and architectures in ways that are easier to understand for humans, without implying that AI systems must resemble the brain in structure or behavior.
2. **Brain as a Guiding Framework**: Others believe that the brain serves as a valuable source of inspiration and guidance. For them, this analogy offers insights and new methodologies that could lead to breakthroughs in AI development by mimicking or adapting biological intelligence more closely.
As you might have guessed, I consider myself in the second camp.

### Fundamentally different training 

Our current training algorithms seem way too inefficient. 
We need to fundamentally rethink what training a network exactly is and how it can be done more efficiently.

Our human brain can learn from a few examples. We don't need thousands of examples and iterations.  Of course, we can learn from a few examples by understanding the context and adapt something new purely on a cognitive level. This doesn't necessarily changes the neuron connections in our brain. 
But eg. in classical, Pavlovian conditioning, we see that conditioning can already work after a few (5-20) training stimuli.

Some other examples that seem to show how many iterations we need to train our brains are:
* Juggling. You learn a trick with a few 10's or iterations (20-100)
* Playing a riff or chord on the piano or guitar.
* Dancing moves and patterns
* Riding a bike
All these examples are things where you don't learn cognitively. It's your 'muscle memory' that learns something new. Which is a sign that you've learned something on a neural level. And it doesn't need 10 000 iterations like we see in neural networks.

Backpropagation / Gradient descent is seen as the most typical training algorithm in neural networks.
As it is currently implemented, it looks way too slow and a bit weird?
Some of the strange things I learned by implementing it myself

- (mini-)batch size looks like an aggregation of training samples to lower the number of calculations. It seems energy-saving to accumulate the changes in 10 or 20 training samples before actually adjusting the weights and biases. But that's not what it's for!
 Backpropagation needs several samples aggregated to be able to compute the necessary changes. This means that it's not possible to add a single item to the knowledge of a neural net. It's only after x training items are added, that the parameters can be adjusted. Simply setting the batch size to 1, doesn't work. I tried it ;-)
- Descent step size is a rather arbitrary number, that is the same over the whole training process, the same in each direction and the same for every neuron. This doesn't feel correct either.

There must be better solutions for this, especially because these 2 parameters have a big impact on training efficiency.

I don't have a better alternative to gradient descent (yet ;-), but there's a broad acceptance that a better solution would be more than welcome. I do think that focusing on the neuron itself instead of the network as a whole might help us in better training.
More on my views on training in a future article (4)

### Non fully connected layers

Part of the reason why AI is so energy-consuming is that it performs a lot of calculations. 
Maybe a lot of these calculations are useless.
The number of calculations is proportional to the number of synapses. 
This number of synapses is proportional to the square of the growth in the number of neurons per layer.
Connecting all neurons from layer i to all neurons in layer i+1 generates a lot of connections. Probably, most of these connections contribute hardly anything to the output produced.
In our brains, neurons don't connect to all other neurons, but 'only' to a few thousand.
The idea that we don't need all these connections is sometimes called 'pruning', or [the Lottery Ticket Hypothesis](https://arxiv.org/abs/1803.03635) where we try to cut away unnecessary connections. In some cases, we can prune away unnecessary neurons as well. 
Although pruning can help improve a neural network, the fact that we talk about 'pruning' still affirms the concept that the fully connected layers are the normal, default case. I'd like to see something more bottom up. Deciding which connections we need instead of deleting the ones we don't need.

Illustration

We could implement non-fully connected layers by setting the weights of the superfluous connections to 0. Unnecessary neurons can be pruned away by setting their bias to 0. 
But in the matrix approach, this doesn't give us any performance advantage. All these zero-connections are still iterated, calculated, and take up memory. Sparse graphs typically make matrix algorithms performance less efficient. 

In our neuron implementations, we can simply drop these connections or neurons altogether, which might lead to significant performance improvements.

Illustration

It also allows us to create layers that are not fully connected with a specific concept in mind. And not just as the negative idea of pruning useless connections.

In image recognition AI, we often use so-called convolutional neural networks. Here, convolutional transformations of the original image are performed outside the neural network, and the results are used as inputs for the network. The convolution is typically a matrix operation as well.
The convolutional step is a way to extract features from the original input (image) that span several pixels.
Using a sort of localized connection, where neurons only connect with neighboring neurons in the next layer, we could implement the convolutional step inside the network itself. We can perform a convolution operation with the exact same Neuron class we used to create our network.

illustration

Again, this approach allows for more flexibility than classical convolution. We could create convolutions that are not uniform over the whole input. Or even convolutions based on a [circular](https://stats.stackexchange.com/questions/351115/convolution-with-a-non-square-kernel/351126#351126) instead of a square kernel. I'm not sure if I can still call these 'convolutions' and 'kernels', but it serves the same purpose. A cell is adjusted by the values of the surrounding cells.
https://docs.google.com/spreadsheets/d/1cPYN4__6qh4jUMAhi05JKhnO_UuzOHB3X7xwENeNj3Q/edit?gid=372625916#gid=372625916
This spreadsheet shows the cells for a certain distance. Most distance values don't generate a square kernel (Values > 4 never generate a square kernel).
You can still implement this as a matrix operation by using 0 values in the square kernel, but then again, you're doing lots of unnecessary calculations.

### No Layers anymore 

We only implemented the layered (fully connected) approach to neural networks because it is computationally easier to handle. It's easier to save/copy/load. It reduces the complexity of the network to 
- an array of neurons per layer (like [5,8,2])
- an array of biases (1 per neuron)
- a (large) array of weights

As we [discussed](https://medium.com/@geertvandamme/78f5714d315c/#a41e) , intelligence is a feature that emerges from complexity, not just from volume. 
If we want to create really intelligent networks, we will need more complexity, instead of sheer volume of parameters.  
In our brains, some neurons are also, locally, somewhat distributed in layers, but it's much less strict. Neurons can make connections to other neurons several layers ahead or with other neurons outside the layered area. Other parts of the brain don't have this layered structure at all.
Why don't we allow connections between neurons from layer 1 to layer 3 or 4?

illustration

The first tests I did are promising, but I'll need some more trials to test where this idea could be helpful.

Or maybe we should just give up on the concept of layers completely. 
There are input neurons, a whole blob of interconnected neurons in between, and output neurons. 

illustration

The network is an acyclic directed graph with input neurons, output neurons, and a hidden part with interconnected neurons. But every neuron only connects to other neurons to the right.
A network like this contains a lot more complexity than a fully connected layered network.
Training and running a network like this still seems possible with our neural implementation without any fundamental changes. 

### Loops or Cycles

If we create just a big blob of interconnected neurons, we might end up with a situation where there appear loops in the network. Neuron A connects to neuron B, which (maybe through neurons C,D,E... ) comes back as an input to neuron A.
This is known as a cyclic directed graph.

Illustration
(Connections in red connect back to neurons that appear earlier in the network and might form a cycle)  

In our own brains, neurons also form these types of loops. 
I think these loops are an essential step in creating more complex, creative neural networks, but it has a significant impact on the algorithms being used.

The previous suggestions (non fully connected and non layered approach) are difficult to integrate in the matrix algorithms. But could be easily handled by our Neuron clas, as long as the network is an Acyclic Directed Graph.
However, once you allow cycles in the interconnected neurons, the whole approach needs to be redesigned fundamentally. 
I guess we need a breadth-first type of forward propagation, and a mechanism for endless loop feedback protection, but it seems possible.

Maybe the recursive algorithms I presented in [the first article](https://medium.com/@geertvandamme/building-an-object-oriented-neural-network-ee3f4af085b6) can be part of the solution.

Long Short-Term Memory (LSTM) and other forms of Recurrent Networks also use some sort of feedback loops, but those cycles are more at the global network level. Not on the level of individual neurons. The inside of the network is still composed of acyclic fully connected layers.  

### Continuity instead of framed input

In the digital realm, we're used to thinking in terms of pixels and frames. But we take this too much for granted. We act like the world itself is quantized the way we're used to see it in our digital world.

- Pictures are made of equally sized pixels
- Sound is split in samples at high frequencies.
- Movies are made up of frames of pictures (of pixels)

And that's the input we feed into our neural networks.
The networks themselves also work in time frames. We feed them one input sample, get an output, and then go on to the next input sample.
Everything is so framed and pixelated.
In the real world, like we experience it, we don't have this framed input. We have continuous input. We see (and hear, and ...) change as a constant flow, not a sequence of discrete frames. 

A nice example of this framed/pixelated concept is [Conway's Game of Life](https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life). 
 It's the ultimate example of pixelation and temporal framing, which is totally different from our 'life'.

I think we might create systems that better interpret real world input if we don't assume a priori how the input should be pixelated, sampled, and framed. Maybe the temporal dimension should in some way be more prevalent than the spacial dimensions in how we encode and interpret video. 
But historically, we developed video as a series of pictures, and that, implicitly, still dictates how we think about reality as seen by a computer. 
Our neural networks are very much framed and pixelated. A lot of effort is spent on how we can pixelate (and quantify) the complex continuous real world input.  
Maybe the more interesting, but challenging question is how we can adapt our neural networks to real continuous input.


## Conclusion

I explained some of the ideas I have in mind for the future.
Some of these proposals are probably totally worthless, others potentially disruptive. But they risk remaining unexplored because of our self-imposed rigidity.
It will take some time and creativity to implement them, but I'll try and keep you posted on the progress, for those who click the Follow button.


illustrations in D3