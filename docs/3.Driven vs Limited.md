# My AI experiments: Driven or Limited by Technology
## The technologies that enabled AI growth might limit it at the same time

The hardware and software technologies which enabled the recent surge in AI development, could themselves become a major limiting factor. If we want our search for intelligent systems to keep on growing, we will need creativity and radically new approaches. This will only be possible if we're willing to question the current conventions and dogmas. 

This article is part of a series on my [AI experiments](https://medium.com/@geertvandamme/61757b11c902), where I explore some challenges I encountered while creating my own, clean, OO Neural network implementation. 

## What seems wrong

In this article in the [series](https://medium.com/@geertvandamme/61757b11c902) I will explore some issues where I think current AI implementations are fundamentally limited.
The way I implemented my own code allows us to deviate from the stereotypical AI designs in a creative and futureproof way.  

## Brute Force Evolution

The recent AI boom is driven by a technological evolution that makes it possible to develop, train, and run the systems we're seeing everywhere now.
The basics of neural networks and the algorithms used to train and these neural networks are a few decades old. It's only now that faster hardware, more memory, cloud computing, and faster GPU's arrived at a level that allows us to create increasingly powerful AI solutions for a whole range of problems.
However, the current AI evolution is largely based on quantitative brute-force progress with bigger and more computers, more GPU's, and [larger training datasets](https://medium.com/predict/ai-is-hitting-a-hard-ceiling-it-cant-pass-851f4667d39b).
This brute force approach leads to exponentially more resource-hungry systems. 
There are some [signs](https://medium.com/predict/ai-is-hitting-a-hard-ceiling-it-cant-pass-851f4667d39b) https://www.youtube.com/watch?v=AqwSZEQkknU  that we're already hitting a limit. The same technology that enabled us to create AI systems is also becoming a hard limiting factor.

## Algorithms and frameworks
I [already mentioned](https://medium.com/@geertvandamme/building-an-object-oriented-neural-network-ee3f4af085b6#c78a) that AI development seems stuck in a local minimum, as we say in typical AI lingo. We made a lot of progress recently, and we know that there's still a lot more room for improvement, but somehow, the current path doesn't seem the most promising. 

If we want a solution to this problem, we'll have to look for qualitative, intelligent growth instead of just brute force.
This will require creative experiments and radical approach changes.
The stereotypical algorithms and frameworks we routinely use, although they delivered some extraordinary results, seem too rigid for real innovation.  
The way we're thinking about AI is based on programming idioms and dogmas that cloud our clear thinking.  
It is my impression that these stereotypes prevent us from being flexible enough to develop radically new, creative architectures and might become a restriction on AI progress.

I guess some major players in AI technology are also researching these alternative approaches. (feel free to contact me ;-)
I didn't find the time yet to explore all these radically new approaches that I have in mind, but I'll try to work on it. Some of these ideas might turn out totally useless, others might offer interesting results, and still some others might be part of the fundamental paradigm changes I mentioned above.

## Restricting factors

During my research, I found that most articles and tutorials are based on a number of assumptions, idioms, and conventions.  
Choosing a very specific implementation because of supposed performance gains comes at a cost of extra inflexibility. Creative, out-of-the-box, alternative directions, some of them probably totally worthless, others potentially disruptive, remain unexplored because of our self-imposed rigidity.

### The default matrix algorithms

When exploring the AI tutorials, it struck me that almost all articles and videos use matrix operations to perform the calculations to train and run a neural network.
At the same time, they use fancy diagrams and images explaining the inner workings of a neural network at the neuron level. 
However, the matrix-based code doesn't reflect the neurological explanation at all. 
This discrepancy is striking. Why not simply have your program mimic the original neurological explanation, like I did in my [OO implementation](https://medium.com/@geertvandamme/building-an-object-oriented-neural-network-ee3f4af085b6)?

![Neuron vs Matrix approach](images/Neuro%20vs%20Matrix.drawio.png)

Especially in the context of a tutorial, your code should reflect your basic explanation and not some incomprehensible mathematical equivalent.
The cognitive dissonance between these 2 supposedly equivalent representations is, in the AI development community, mostly solved by appreciating the matrix view as the real solution, the way 'real' developers think. The neuron approach becomes just an illustration. A way of explaining neural networks to people less programming-experienced.
However, it really is the other way around. The basic building blocks of neural networks are the neurons. The matrices are a mathematical projection of the original idea. 

Of course, I know, the matrix algorithms are supposed to give us superior performance. 

Except....  They don't. And they come with their own set of disadvantages.

At least not directly. It only boosts performance because we have libraries that can offload the matrix multiplication to a GPU. (Or in the case of Python, because we have libraries that can offload the calculations to another, decent, programming language.)  

As controversial as it sounds, the matrix algorithms offer no real advantage on their own. 
Especially in the context of tutorials, where raw performance is not a real issue, we should focus on clean and understandable code.

```python
    def feedforward(self, a):
        """Return the output of the network if ``a`` is input."""
        for b, w in zip(self.biases, self.weights):
            a = sigmoid(np.dot(w, a)+b)
        return a
```
This isn't exactly an example of clean readable code. It favors a programming culture of using supposedly clever tricks that may seem intelligent and handy to experienced AI programmers, but are in fact obscuring the real operations. Apart from the bad variable names, the code is also making the basic concepts of layers and neurons implicit. Nothing in the code represents the neuron or layer concept. 

Starting off with a matrix-based implementation, only because it's supposed to make our code run faster, is a clear example of premature optimization.


> "Premature Optimization is the Root of All Evil"
> 
> -Donald Knuth


Of course, the matrix approach isn't useless in AI, as demonstrated in about every current real world AI system. It allows us to execute our calculations on specialized hardware. The performance boost that this hardware delivers can be necessary to come to a workable real-life solution.
But the fact that the matrix algorithms are presented as the default, or even the only viable option, blinds us from other, more flexible, creative alternatives.

### Python as the default AI language

Pythonâ€™s simplicity, extensive library ecosystem, and large community support have helped it become the dominant programming language for AI and data analysis.
Python is a language often favored and taught in academia.

However, Python has notable drawbacks when applied to computationally intensive tasks like training and deploying large neural networks.
The language is slow, ugly (a tiny bit subjective), and, despite [it was initially developed to make programs more readable](https://peps.python.org/pep-0020/), often encourages bad programming habits.
Python favors or even encourages quick and dirty code. At first, that might seem like a good thing for a more creative experimental approach.
But neural networks are computationally intensive, and python is so slow, it quickly forces us to use external libraries and frameworks. 
The reliance on external libraries such as NumPy and TensorFlow has undoubtedly accelerated AI development. However, it makes us dependent on fixed design decisions and abstracts away details, which limits our ability to innovate.

The use of python as the default language for AI development brings along a specific rigid, inflexible [programming culture](https://gist.github.com/RobertAKARobin/a1cba47d62c009a378121398cc5477ea).

> "That's the way we do things over here" is never a good mindset for innovation.

I'm not saying you should use Rust, Go, Java, Node ... instead of python. In the ideal situation, you should pick the language that best suits your needs in a particular case. The fact that python is so prevalent in AI is just unfortunate.

From an educational perspective, it would even be interesting to implement a neural network in SQL, or Excel.

### Aren't GPU's meant for gaming?

As discussed before, we use GPU's because they allow us to perform a certain type of calculations much faster, especially in large networks. 
GPU's absolutely have an immense value in AI development, since they can perform a lot of calculations in parallel.
But it feels wrong when we limit our algorithmic choices based on an architectural hardware preconception.

Using the extra GPU power isn't limited to just matrix multiplications. Other operations can also be performed on GPU's, as long as we can do them in parallel. 
It's highly probable that we can still use this power to perform the neuron-oriented calculations. But that's something I want to explore later.  

## Radical changes

What exactly do I mean by the fundamental creative changes I hinted at in the previous section?

- My future experiments
- I have some more ;-)  (integers eg)
- Brain analogy serves as a guide sometimes
- paradigm
- We discussed some design pitfalls that kept us from exploring new ideas.
- What are the options if we have a more flexible basic design?

### Fundamentally different training 

Our current training algorithms seem way too inefficient. 
We need to fundamentally rethink what training a network exactly is and how it can be done more efficiently.

Our human brain can learn from a few examples. We don't need hundreds of examples and iterations.  Of course, we can learn from a few examples by understanding the context. This doesn't necessarily changes the neuron connections in our brain. But in classical, Pavlovian conditioning, we see that conditioning already works after a few (5-20) training stimuli.

Some other examples that seem to show how many iterations we need to train our brains are:
* Juggling. You learn a trick with a few 10's or iterations (20-100)
* Playing a riff or chord on the piano or guitar.
* Dancing moves and patterns
* Riding a bike
All these examples are things where you don't learn cognitively. It's your 'muscle memory' which is a sign that you've learned something on a neural level. And it doesn't need 10 000 iterations.

Backpropagation / Gradient descent is seen as the most typical training algorithm in neural networks.
As it is currently implemented, it looks way too slow and a bit weird?
Some of the strange things I learned by implementing it myself
- (mini-)batch size looks like an aggregation of training samples to lower the number of calculations. It seems energy-saving to accumulate the changes in 10 or 20 training samples before actually adjusting the weights and biases. But that's not what it's for!
 Backpropagation needs several samples aggregated to be able to compute the necessary changes. This means that it's not possible to add a single item to the knowledge of a neural net. It's only after x training items are added, that the parameters can be adjusted. Simply setting the batch size to 1, doesn't work. I tried it ;-)
- Descent step size is a rather arbitrary number, that is the same over the whole training process, the same in each direction and the same for every neuron. That doesn't feel correct either.

There must be better solutions for this, especially because these 2 parameters have a big impact on training efficiency.

I don't have a better alternative to gradient descent (yet ;-), but there's a broad acceptance that a better solution would be more than welcome. I do think that focusing on the neuron itself instead of the network as a whole might help us in better training.
More on my views on training in a future article (4)

### Non fully connected layers

Part of the reason why AI is so energy-consuming is that it performs a lot of calculations. 
Maybe a lot of these calculations are useless.
The number of calculations is proportional to the number of synapses. 
This number of synapses is proportional to the square of the growth in the number of neurons per layer.
Connecting all neurons from layer i to all neurons from layer i+1 generates a lot of connections. Probably, most of these connections contribute hardly anything to the output produced.
In our brains, neurons don't connect to all other neurons, but 'only' to a few thousand.
The idea that we don't need all these connections is often called 'pruning', or [the Lottery Ticket Hypothesis](https://arxiv.org/abs/1803.03635) where we cut away unnecessary connections. In some cases, we can prune away unnecessary neurons as well. 
Although pruning can help improve a neural network, the fact that we talk about 'pruning' still affirms the concept that the fully connected layers are the normal, default case. I'd like to see something more bottom up. Deciding which connections we need instead of deleting the ones we don't need.

We could implement non-fully connected layers by setting the weights of the non-existing connections to 0. Unnecessary neurons can be pruned away by setting their bias to 0. 
But in the matrix approach, this doesn't give us any performance advantage. All these zero-connections are still iterated, calculated, and take up memory. Sparse matrices typically make matrix algorithms perform less efficient . 

In our neuron implementations, we can simply drop these connections or neurons altogether, which might lead to significant performance improvements.

Illustration

It also allows us to create layers that are not fully connected with a specific concept in mind. And not just as the negative idea of pruning useless connections.

In image recognition AI, we often use so-called convolutional neural networks. Here, convolutional transformations of the original image are performed outside the neural network, and the results are used as inputs for the network. The convolution is typically a matrix operation as well.
I think (but would need more experimentation) that, using a sort of localized connection, where neurons only connect with neighboring neurons in the next layer, could implement the convolutional step inside the network. We can perform a convolution operation with the exact same Neuron class we used to create our network.

illustration

Again, this approach allows for more flexibility. We could create convolutions that are not uniform over the whole input. Or even convolutions based on [circular](https://stats.stackexchange.com/questions/351115/convolution-with-a-non-square-kernel/351126#351126)  instead of a square kernel. I'm not sure if I can still call these convolutions and kernels, but it serves the same purpose. A cell is adjusted by the values of the surrounding cells.
https://docs.google.com/spreadsheets/d/1cPYN4__6qh4jUMAhi05JKhnO_UuzOHB3X7xwENeNj3Q/edit?gid=372625916#gid=372625916
This spreadsheet shows the cells for a certain distance. Most distance values don't generate a square kernel (Values > 4 never generate a square kernel).
You can still implement this as a matrix operation by using 0 values in the kernel, but then again, you're doing lots of unnecessary calculations.

### No Layers anymore 

We only implemented the layered (fully connected) approach to neural networks because it is computationally easier to handle. It's easier to save/copy/load. It reduces the complexity of the network to 
- an array of neurons per layer (like [5,8,2])
- an array of biases
- an array of weights

As we discussed in (2), intelligence is a feature that emerges from complexity. 
If we want to create really intelligent networks, we will need more complexity, instead of sheer volume of parameters.  
In our brains, some neurons are also, locally, somewhat distributed in layers, but it's much less strict. Neurons can make connections to other neurons several layers ahead or with other neurons outside the layered location. 
Why don't we allow connections between neurons from layer 1 to layer 3 or 4?

illustration

Or maybe we should just give up on the concept of layers completely. 
There are input neurons, a whole blob of interconnected neurons in between, and output neurons. 

illustration

The network is an acyclic directed graph with input neurons, output neurons, and a hidden part with interconnected neurons. But every neuron only connects to other neurons to the right.
Training and running a network like this is still possible with our OO implementation. 

### Loops or Cycles

Of course, if we have a blob of interconnected neurons, there might be a situation where there appear loops in the network. Neuron A connects to neuron B, which (maybe through neurons C,D,E... ) comes back as an input to neuron A.
This is known as a cyclic directed graph.

Illustration
(Connections in red connect back to neurons that appear earlier in the network and might form a loop)  

In our own brains, neurons also form these types of loops. 
I think these loops are an essential step in creating more complex, creative neural networks, but it has a significant impact on the algorithms being used.

The previous suggestions (non fully connected and no layered approach) are difficult to integrate in the matrix algorithms, but could be easily handled by the Neuron clas I proposed in (1), as long as the network is an Acyclic Directed Graph.
However, once you allow cycles in the interconnected neurons, the whole approach needs to be redesigned fundamentally. 
I guess we need a breadth-first type of forward propagation, and a mechanism for endless loop feedback protection.

Maybe the recursive algorithms I presented in (1) can be part of the solution.

Long Short-Term Memory (LSTM) and other forms of Recurrent Networks use some sort of feedback loops, but those are more at the global network level. Not on the level of individual neurons. The inside of the network is still composed of acyclic fully connected layers.  

### Continuity instead of framed input

- Pictures make of pixels
- sound is split in samples
- movies are made up of frames (which in turn are made up of pixels), which gives the temporal dimension a lower priority over the 2 spatial dimensions. The same pixel in 2 different frames are more apart than 2 adjacent pixels in 1 frame.  
- Convolutions over 3 dimensions
- https://www.sciencedirect.com/science/article/abs/pii/S0923596516300935

I have the impression that there is research going on about these things (non-fully connected, continuous ...)  but that is mainly focused on creating specialized hardware.

## Conclusion

Some of the critique may seem a bit hard and drastic.
Some solutions might seem far-fetched.
I fully understand that it is now up to me to come up with practical implementations where I show the possible benefits of the proposed alternatives. 
