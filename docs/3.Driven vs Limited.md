# My AI experiments: Driven vs Limited by technology
## The technologies that enabled AI growth, might limit it at the same time

This article is part of a series on my [AI experiments](https://medium.com/@geertvandamme/61757b11c902)....


The recent AI boom is driven by a technological evolution that makes it possible to develop, train and run the systems we're seeing now.  
Faster hardware, bigger memory, cloud computing, GPU's allow us to create powerful AI solutions for a whole range of problems.
However, the current AI evolution is largely based on quantitative brute-force progress with bigger and more computers, more GPU's, and larger training datasets.
This brute force approach leads to exponentially more resource-hungry systems. 
There are some [signs](https://medium.com/predict/ai-is-hitting-a-hard-ceiling-it-cant-pass-851f4667d39b) https://www.youtube.com/watch?v=AqwSZEQkknU  that we're already hitting a limit. The same technology that enabled us to create AI systems is also becoming the limiting factor. (AGI Articles)
The main concerns here are energy (link) and training data... (uitwerken)

## Algorithms and frameworks
In ['Building an Object Oriented Neural Network'](https://medium.com/@geertvandamme/building-an-object-oriented-neural-network-ee3f4af085b6) I already mentioned that AI development seems stuck in a local minimum.
If we want a solution to this problem, we'll have to look for qualitative, intelligent growth instead of brute force.
This will require creative experiments and radical approach changes.
Just like the hardware, the current algorithms and frameworks are too rigid. 
They are not flexible enough to allow us to develop radically new, creative solutions and are also becoming a limiting factor.

I'm convinced that the major players in AI technology are also looking for these alternative approaches. 
I didn't yet find enough time to explore these radically new approaches, but I have some ideas. Some of these ideas might turn out totally useless, others might offer interesting results, and still some others might be part of the fundamental changes I mentioned above.

### Drop the matrix algorithms

...
No real advantage on its own, but it will allow us to explore other directions. more broadly.  
Don't like matrix approach in graph algorithms either. 

Oh, and by the way, drop python as the default language for data analysis and AI.
Or even better, simply drop python ;-)

### Fundamentally different training 

Our current training algorithms seem way too inefficient. 
We need to fundamentally rethink what training a network exactly is, and how it can be done most efficiently.
Gradient descent, as it is currently used, looks way too slow.
Batch size and descent step size are 2 concepts that don't really feel right.

Batch size means that it's not possible to add a single item to the knowledge of a neural net. It's only after x training items are added, that the parameters are adjusted.
OTOH, simply setting the batch size to 1, doesn't work. I tried it ;-)

Descent step size is a rather arbitrary number, that is the same over the whole training process and is the same in each direction. That doesn't feel correct either.

These 2 remarks show a fundamental issue with the current training approach in neural networks.
Our human brain can learn from a single (or a few) examples. We don't need hundreds of examples and iterations.
More on training in (4)

### Non fully connected layers

Part of the reason why AI is so energy consuming is that it performs a lot of calculations. 
Maybe a lot of these calculations are useless.
The number of calculations is proportional to the number of synapses. 
This number of synapses is proportional to the square of the growth in the number of neurons.
In our brain, neurons don't connect to all other neurons, but 'only' to a few thousand. 
There are techniques where connections between neurons are dropped out by setting the weight to 0. 
https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf
But in the matrix approach, this doesn't give us any performance advantage. Since the calculation will be performed anyway, even if it is eventually multiplied by 0.
In my OO approach, a connection with (near) 0 weight could be really dropped.

Convolutional 

### Stop using layers at all 

We only implemented the layered approach to neural networks, because it is computationally easier to handle. (again the matrix approach)
In our brain, in some way, neurons are also located in different layers, but it's less strict. Neurons can make connections to other neurons several layers ahead. 
Why don't we allow connections between neurons from layer 1 to layer 3 or 4?
Or maybe we should just give up on the concept of layers completely. 
There are input neurons, a whole blob of interconnected neurons in between, and output neurons.

### And loops

Of course, if we have a blob of interconnected neurons, there might/will be loops. 
In our own brains, neuron connections also form loops. 
I think these loops are an essential step in creating more creative neural networks, but that has a significant impact on the algorithms being used. 
The previous proposals already have a serious impact on the training algorithm. Allowing loops makes this much more complex. 
Loops need a total rework of the forward propagation algorithm as well. 
The previous suggestions (non fully connected and no layered approach) are difficult to integrate in the matrix algorithms, but could be easily handled by the classes I proposed in (1).
However, once you allow loops in the interconnected neurons, the whole approach needs to be redesigned fundamentally. 
I guess we need a breadth-first type of forward propagation, and a mechanism for endless loop feedback protection.
Maybe the recursive algorithms I presented in (1) can be part of the solution.

Long short-term memory (LSTM) networks use feedback loops, but that's more on the network level. Not on the level of individual neurons.

https://medium.com/predict/ai-is-hitting-a-hard-ceiling-it-cant-pass-851f4667d39b
