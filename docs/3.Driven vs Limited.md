# My AI experiments: Driven or Limited by Technology
## The technologies that enabled AI growth might limit it at the same time


The hardware and software technologies which enabled the recent surge in AI advancements, could themselves become a major limiting factor. If we want our search for intelligent systems to keep on growing, we will need creativity and radically new approaches. This will only be possible if we're willing to question the current conventions and dogmas. 

This article is part of a series on my [AI experiments](https://medium.com/@geertvandamme/61757b11c902), where I explore some challenges I encountered while creating my own, clean, OO Neural network implementation. 

## What seems wrong

In this article in the [series](https://medium.com/@geertvandamme/61757b11c902) I will explore some issues where I think current AI implementations are fundamentally limited. 
The way I implemented my own code allows us to deviate from the stereotypical AI designs in a creative and futureproof way. 

## Brute Force Evolution

The recent AI boom is driven by a technological evolution that makes it possible to develop, train, and run the systems we're seeing everywhere now.
The basics of neural networks and the algorithms used to train and these neural networks are a few decades old. It's only now that faster hardware, more memory, cloud computing, and faster GPU's arrived at a level that allows us to create increasingly powerful AI solutions for a whole range of problems.
However, the current AI evolution is largely based on quantitative brute-force progress with bigger and more computers, more GPU's, and larger training datasets.
This brute force approach leads to exponentially more resource-hungry systems. 
There are some [signs](https://medium.com/predict/ai-is-hitting-a-hard-ceiling-it-cant-pass-851f4667d39b) https://www.youtube.com/watch?v=AqwSZEQkknU  that we're already hitting a limit. The same technology that enabled us to create AI systems is also becoming a hard limiting factor.

## Algorithms and frameworks
I [already mentioned](https://medium.com/@geertvandamme/building-an-object-oriented-neural-network-ee3f4af085b6) that AI development seems stuck in a local minimum, as we say in typical AI lingo. We made a lot of progress recently, and we know that there's still a lot more room for improvement, but somehow, the current path doesn't seem the most promising. 

If we want a solution to this problem, we'll have to look for qualitative, intelligent growth instead of just brute force.
This will require creative experiments and radical approach changes.
The stereotypical algorithms and frameworks we routinely use, although they delivered some extraordinary results, seem too rigid for real innovation.  
The way we're thinking about AI is based on programming idioms and dogmas that cloud our clear thinking.  
It is my impression that these stereotypes prevent us from being flexible enough to develop radically new, creative architectures and might become a restriction on AI progress.

I guess some major players in AI technology are also researching these alternative approaches. (feel free to contact me ;-)
I didn't find the time yet to explore all these radically new approaches that I have in mind, but I'll try to work on it. Some of these ideas might turn out totally useless, others might offer interesting results, and still some others might be part of the fundamental changes I mentioned above.

## Restricting factors

During my research, I found that most articles and tutorials are based on a number of  assumptions, idioms, and conventions.  
Choosing a very specific implementation because of supposed performance gains comes at a cost of extra inflexibility. Creative, out-of-the-box, alternative directions, some of them probably totally worthless, others potentially disruptive, remain unexplored because of our self-imposed rigidity.

### Drop the default matrix algorithms

When exploring the AI tutorials, it struck me that almost all articles and videos use matrix operations to perform the calculations to train and run a neural network.
At the same time, they use fancy diagrams and images explaining the inner workings of a neural network at the neuron level. 
However, the matrix-based code doesn't reflect the neurological explanation at all. 
This discrepancy is striking. Why not simply have your program mimic the original neurological explanation, like I did in my [OO implementation](link)?

TODO: illustration![ChatGPT Image Jun 5, 2025, 04_13_02 PM.png](images/ChatGPT%20Image%20Jun%205%2C%202025%2C%2004_13_02%20PM.png)

Especially in the context of a tutorial, your code should reflect your basic explanation and not some incomprehensible mathematical equivalent.
The cognitive dissonance between these 2 supposedly equivalent representations is, in the AI development community, mostly solved by appreciating the matrix view as the real solution, the way 'real' developers think. The neuron approach becomes just an illustration. A way of explaining neural networks to people less programming-experienced.
However, it really is the other way around. The basic building blocks of neural networks are the neurons. The matrices are a mathematical projection of the original idea. 

Of course, I know, the matrix algorithms are supposed to give us superior performance. 

Except....  They don't. (And they come with their own set of disadvantages.)

At least not directly. It only boosts performance because we have libraries that can offload the matrix multiplication to a GPU. (Or in the case of Python, because we have libraries that can offload the calculations to another, decent, programming language.)  

As controversial as it sounds, the matrix algorithms offer no real advantage on their own. 
Especially in the context of tutorials, where raw performance is not a real issue, we should focus on clean and understandable code.

```python
    def feedforward(self, a):
        """Return the output of the network if ``a`` is input."""
        for b, w in zip(self.biases, self.weights):
            a = sigmoid(np.dot(w, a)+b)
        return a
```
This isn't exactly an example of clean readable code. It favors a programming culture of using supposedly clever tricks that may seem intelligent and handy to experienced AI programmers, but are in fact obscuring the real operations. Apart from the bad variable names, the code is also making the basic concepts of layers and neurons implicit.  

Starting off with a matrix-based implementation, only because it's supposed to make our code run faster, is a clear example of premature optimization.


> "Premature Optimization is the Root of All Evil"
> 
> -Donald Knuth


Of course, the matrix approach isn't useless in AI, as demonstrated in about every current real world AI system. It allows us to execute our calculations on specialized hardware. The performance boost that this hardware delivers can be necessary to come to a workable real-life solution.
But the fact that the matrix algorithms are presented as the default, or even the only viable option, blinds us from other, more flexible, creative alternatives.

### Drop Python as the default AI language

Pythonâ€™s simplicity, extensive library ecosystem, and large community support have helped it become the dominant programming language for AI and data analysis.
Python is a language often favored and taught in academia.

However, Python has notable drawbacks when applied to computationally intensive tasks like training and deploying large neural networks.
The language is slow, ugly (a tiny bit subjective), and, despite [it was initially developed to make programs more readable](https://peps.python.org/pep-0020/), often encourages bad programming habits.
Python favors or even encourages quick and dirty code. At first, that might seem like a good thing for a more creative experimental approach.
But neural networks are computationally intensive, and python is so slow, it quickly forces us to use external libraries and frameworks. 
The reliance on external libraries such as NumPy and TensorFlow has undoubtedly accelerated AI development. However, it also abstracts away many details, which, while reducing the learning curve, may limit our ability to innovate at the algorithmic level.

The use of python as the default language for AI development brings along a specific rigid, inflexible [programming culture](https://gist.github.com/RobertAKARobin/a1cba47d62c009a378121398cc5477ea).

> "That's the way we do things over here" is never a good mindset for innovation.

I'm not saying you should use Rust, Go, Java, Node ... instead of python. In the ideal situation, we should pick the language that best suits our needs in a particular case. The fact that python is so prevalent in AI is just unfortunate.
From an educational perspective, it would even be interesting to implement a neural network in SQL, or Excel.

### Aren't GPU's meant for gaming?

As discussed before, we use GPU's because they allow us to perform a certain type of calculations much faster, especially in large networks. (in parallel)
GPU's absolutely have an immense value in AI development. I wouldn't argue that NVidia would be totally worthless.... 
But it feels wrong when we limit our algorithmic choices based on an architectural hardware preconception. (prejudice)

Compromise
eg. Pruning, parallelization

Using the extra GPU power isn't limited to matrix algorithms. We might find other uses of the possibilities that they give us. Commom meeting ground.  


## Radical changes

- What are the fundamental creative changes?
- My future experiments
- I have some more ;-)  (integers eg)
- Brain analogy serves as a guide sometimes
- paradigm
- We discussed some design pitfalls that kept us from exploring new ideas.
- What are the options if we have a more flexible basic design?

### Fundamentally different training 

Our current training algorithms seem way too inefficient. 
We need to fundamentally rethink what training a network exactly is and how it can be done more efficiently.
Gradient descent, as it is currently used, looks way too slow and a bit weird?.
Batch size and descent step size (learning rate) are 2 concepts in plain vanilla gradient descent that don't really feel right. Especially when they are fixed at the network level. 

What felt weird / simplistic From my own implementation:

Batch size means that it's not possible to add a single item to the knowledge of a neural net. It's only after x training items are added, that the parameters are adjusted.
OTOH, simply setting the batch size to 1, doesn't work. I tried it ;-)

Descent step size is a rather arbitrary number, that is the same over the whole training process and is the same in each direction. That doesn't feel correct either.

Our human brain can learn from a few examples. We don't need hundreds of examples and iterations.  Of course, we can learn from a few examples by understanding the context. This doesn't necessarily changes the neuron connections in our brain. But in classical, Pavlovian conditioning, we see that the conditioning already works after a few (5-20) training stimuli.

Juggling / muscle brain / happens too fast to overthink it. 
A juggler doesn't need thousands of training samples to learn a new trick. Only a few 10's
riding a bike / driving a car / playing a certain riff or chord on a piano or guitar. 

I don't have a better alternative to gradient descent (yet ;-), but there's a broad acceptance that a better solution would be more than welcome.
More on my views on training in another article (4)

### Non fully connected layers

Part of the reason why AI is so energy consuming is that it performs a lot of calculations. 
Maybe a lot of these calculations are useless.
The number of calculations is proportional to the number of synapses. 
This number of synapses is proportional to the square of the growth in the number of neurons.
Connecting all neurons from layer i to all neurons from layer i+1 generates a lot of connections. Perhaps most of these connections contribute nothing to the output.
In our brain, neurons don't connect to all other neurons, but 'only' to a few thousand.
This technique is often called 'pruning', where we cut away unnecessary connections. In some cases, we can prune away unnecessary neurons as well.
Although pruning can help improve a neural network, the fact that we talk about 'pruning' still affirms the concept that the fully connected layers are the normal, default case. 

We could implement non-fully connected layers by setting the weights of the non-existing connections to 0. Unnecessary neurons can be pruned away by setting the bias to 0. 
But in the matrix approach, this doesn't give us any performance advantage. Sparse matrices might lead to less efficient matrix algorithms. All these zero-connections are still iterated, calculated, and take up memory.

In the neuron implementations, we can simply drop these connections or neurons altogether, which might lead to significant performance improvements.

Illustration

It also allows us to create layers that are not fully connected with a specific concept in mind. And not just as the negative idea of pruning useless connections.

In image recognition AI, we often use so-called convolutional neural networks. Here, convolutional transformations of the original image are performed outside the neural network, and the results are used as inputs for the network. I think (but would need more experimentation) that, using a sort of localized connection, where neurons only connect with neighboring neurons in the next layer, could implement the convolutional step inside the network. We can perform a convolution operation with the exact same Neuron class we used to create our network.

illustration

Again, this approach allows for more flexibility. We could create convolutions that are not uniform over the whole input. Or even convolutions based on [circular](https://stats.stackexchange.com/questions/351115/convolution-with-a-non-square-kernel/351126#351126)  instead of a square kernel. I'm not sure if I can still call these convolutions and kernels, but it serves the same purpose. A cell is adjusted by the values of the surrounding cells.
https://docs.google.com/spreadsheets/d/1cPYN4__6qh4jUMAhi05JKhnO_UuzOHB3X7xwENeNj3Q/edit?gid=372625916#gid=372625916
This spreadsheet shows the cells for a certain distance. Most distance values don't generate a square kernel (Values >4 never generate a square kernel).
You can still implement this in the matrix approach by using 0 values, but again, you're doing lots of unnecessary calculations.

### Stop using layers at all 

We only implemented the layered approach to neural networks, because it is computationally easier to handle. (again the matrix focus)
In our brain, some neurons are also somewhat distributed in layers, but it's much less strict. Neurons can make connections to other neurons several layers ahead or even with other neurons outside the layered location. 
Why don't we allow connections between neurons from layer 1 to layer 3 or 4?

illustration

Or maybe we should just give up on the concept of layers completely. 
There are input neurons, a whole blob of interconnected neurons in between, and output neurons.

illustration

### And loops or cycles

Of course, if we have a blob of interconnected neurons, there might/will be loops. Neuron A connects to neuron B, which (maybe through neurons C,D,E... ) comes back as an input to neuron A.
feedback

Illustration

In our own brains, neuron connections also form these types of loops. 
I think these loops are an essential step in creating more complex, creative neural networks, but that has a significant impact on the algorithms being used. 
The ~~previous proposals~~ already have a serious impact on the training algorithm. Allowing loops makes this much more complex. 
Loops need a total rework of the forward propagation algorithm as well. 
The previous suggestions (non fully connected and no layered approach) are difficult to integrate in the matrix algorithms, but could be easily handled by the classes I proposed in (1), as long as the network is an Directed Acyclic Graph.
However, once you allow cycles in the interconnected neurons, the whole approach needs to be redesigned fundamentally. 
I guess we need a breadth-first type of forward propagation, and a mechanism for endless loop feedback protection.
setTimeout(f,0) or yield

Maybe the recursive algorithms I presented in (1) can be part of the solution.

Long short-term memory (LSTM) networks use some sort of feedback loops, but those are more at the global network level. Not on the level of individual neurons. The inside of the network is still composed of acyclic fully connected layers.  

https://medium.com/predict/ai-is-hitting-a-hard-ceiling-it-cant-pass-851f4667d39b


### Continuity instead of framed input

- Pictures make of pixels
- sound is split in samples
- movies are made up of frames (which in turn are made up of pixels), which gives the temporal dimension a lower priority over the 2 spatial dimensions. The same pixel in 2 different frames are more apart than 2 adjacent pixels in 1 frame.  
- Convolutions over 3 dimensions
- https://www.sciencedirect.com/science/article/abs/pii/S0923596516300935

I have the impression that there is research going on about these things (non-fully connected, continuous ...)  but that is mainly focused on creating specialized hardware.

## Conclusion

Some of the critique may seem a bit hard and drastic.
Some solutions might seem far-fetched.
I fully understand that it is now up to me to come up with practical implementations where I show the possible benefits of the proposed alternatives. 